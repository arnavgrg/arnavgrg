### Hi there ðŸ‘‹

I'm a Machine Learning Engineer at Predibase, working on large language models and distributed training. I love working on open-source projects. ðŸš€

I maintain [Ludwig](https://github.com/ludwig-ai/ludwig.git), an open-source toolbox for low-code/no-code deep learning. 

Some of my recent work includes:

1. Using reinforcement fine-tuning to generate Triton kernels from PyTorch code snippets. [Read More](https://predibase.com/blog/teaching-ai-to-write-gpu-code-a-deep-dive-into-reinforcement-fine-tuning)
2. Co-creating Turbo LoRA, a parameter efficient training method to jointly fine-tune for quality and speculative decoding, improving throughput by upto 3.5x for fine-tuned LoRA adapters. [Read More](https://predibase.com/blog/turbo-lora)
3. Developing a mixture of agents synthetic data generation algorithm for supervised instruction tuning that can beat K-shot GPT-4o using just 10 rows of data. [Read More](https://predibase.com/blog/how-to-generate-synthetic-data-and-fine-tune-a-slm-that-beats-gpt-4o)
4. Co-authoring LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report. Read the [paper](https://arxiv.org/abs/2405.00732).
5. Speeding up LLM training by 15x. [Read More](https://predibase.com/blog/how-we-accelerated-fine-tuning-by-15x-in-less-than-15-days)

You can connect with me on [LinkedIn](https://www.linkedin.com/in/arnavgrg) to discuss all things AI ðŸ¤–.

If you like my work and want to support it, feel free to buy me a coffee â˜•: https://buymeacoffee.com/arnavgarg 
